{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dbcfff8-4eee-41b7-b8c8-9d29e95d74c0",
   "metadata": {},
   "source": [
    "## Get the image link and names from google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1945aef-acdb-408f-8286-a854cb02c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from queue import Queue\n",
    "from io import BytesIO\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image, ImageFile\n",
    "import pytesseract\n",
    "import gdown\n",
    "# Google Drive and Auth\n",
    "from pydrive2.auth import GoogleAuth as GoogleAuth2\n",
    "from pydrive2.drive import GoogleDrive as GoogleDrive2\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.auth.transport.requests import Request\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "import google.auth\n",
    "from google.auth.transport.requests import AuthorizedSession\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import build_http\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84d08964-d3df-4aca-bc0b-216037a696e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scopes and folder ID\n",
    "SCOPES = [\"https://www.googleapis.com/auth/drive\"]\n",
    "FOLDER_ID_2016_2017 = \"0AB47zEhxmVHPUk9PVA\"  \n",
    "FOLDER_ID_2017_2018 = \"0ALVHtXWwU7NPUk9PVA\"\n",
    "FOLDER_ID_2018_2019_1 = \"0AAV-GgAjGbW2Uk9PVA\"\n",
    "FOLDER_ID_2018_2019_2 = \"0AKW1tx0XOaaJUk9PVA\"\n",
    "\n",
    "# # Authenticate\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.credentials = ServiceAccountCredentials.from_json_keyfile_name('../../credentials.json', SCOPES)\n",
    "# drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "679bcb9b-04e9-4a0a-a9f4-33d9b24168df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Global Variables ===\n",
    "# SCOPES = ['https://www.googleapis.com/auth/drive.readonly']\n",
    "# folder_queue = Queue()\n",
    "# final_links = []  # Now a list of records instead of nested dict\n",
    "\n",
    "# # === Step 1: Authentication ===\n",
    "# def authenticate_drive_api():\n",
    "#     creds = None\n",
    "#     if os.path.exists('token.pickle'):\n",
    "#         with open('token.pickle', 'rb') as token:\n",
    "#             creds = pickle.load(token)\n",
    "#     if not creds or not creds.valid:\n",
    "#         if creds and creds.expired and creds.refresh_token:\n",
    "#             creds.refresh(Request())\n",
    "#         else:\n",
    "#             flow = InstalledAppFlow.from_client_secrets_file('client_secrets.json', SCOPES)\n",
    "#             creds = flow.run_local_server(port=0)\n",
    "#         with open('token.pickle', 'wb') as token:\n",
    "#             pickle.dump(creds, token)\n",
    "#     service = build('drive', 'v3', credentials=creds)\n",
    "#     return service\n",
    "\n",
    "# # === Step 2: Worker Function ===\n",
    "# def worker():\n",
    "#     service = authenticate_drive_api()\n",
    "#     while True:\n",
    "#         context = folder_queue.get()\n",
    "#         if context is None:\n",
    "#             break\n",
    "#         crawl_folder(service, context)\n",
    "#         folder_queue.task_done()\n",
    "\n",
    "# # === Step 3: Folder Crawler ===\n",
    "# def crawl_folder(service, context):\n",
    "#     folder_id, current_path = context  # (folder_id, full_path)\n",
    "\n",
    "#     query = f\"'{folder_id}' in parents and trashed=false\"\n",
    "#     page_token = None\n",
    "#     while True:\n",
    "#         response = service.files().list(\n",
    "#             q=query,\n",
    "#             spaces='drive',\n",
    "#             fields='nextPageToken, files(id, name, mimeType)',\n",
    "#             supportsAllDrives=True,\n",
    "#             includeItemsFromAllDrives=True,\n",
    "#             pageToken=page_token\n",
    "#         ).execute()\n",
    "\n",
    "#         for file in response.get('files', []):\n",
    "#             mime_type = file['mimeType']\n",
    "#             file_id = file['id']\n",
    "#             title = file['name']\n",
    "\n",
    "#             if mime_type == 'application/vnd.google-apps.folder':\n",
    "#                 # It's a folder: recurse into it\n",
    "#                 next_path = os.path.join(current_path, title)\n",
    "#                 folder_queue.put((file_id, next_path))\n",
    "\n",
    "#             elif mime_type.startswith('image/'):\n",
    "#                 view_link = f\"https://drive.google.com/file/d/{file_id}/view\"\n",
    "#                 final_links.append({\n",
    "#                     \"filename\": title,\n",
    "#                     \"filepath\": view_link,\n",
    "#                     \"directory\": current_path.replace(\"\\\\\", \"/\")  # Make sure slashes are correct\n",
    "#                 })\n",
    "\n",
    "#         page_token = response.get('nextPageToken', None)\n",
    "#         if page_token is None:\n",
    "#             break\n",
    "\n",
    "# # === Step 4: Collect Image Links in Parallel ===\n",
    "# def collect_image_links_parallel(start_folder_id, num_threads=10):\n",
    "#     threads = []\n",
    "#     for _ in range(num_threads):\n",
    "#         t = threading.Thread(target=worker)\n",
    "#         t.start()\n",
    "#         threads.append(t)\n",
    "\n",
    "#     # Start with root folder (path is empty string)\n",
    "#     folder_queue.put((start_folder_id, \"\"))\n",
    "\n",
    "#     folder_queue.join()\n",
    "\n",
    "#     # Stop workers\n",
    "#     for _ in threads:\n",
    "#         folder_queue.put(None)\n",
    "#     for t in threads:\n",
    "#         t.join()\n",
    "\n",
    "#     return final_links\n",
    "\n",
    "# # === Step 5: Main Execution ===\n",
    "# if __name__ == \"__main__\":\n",
    "#     service = authenticate_drive_api()\n",
    "\n",
    "#     # Your Folder ID\n",
    "#     FOLDER_ID_2018_2019_2 = \"0AKW1tx0XOaaJUk9PVA\"\n",
    "\n",
    "#     all_image_links = collect_image_links_parallel(FOLDER_ID_2018_2019_2)\n",
    "\n",
    "#     print(f\"\\nâœ… Found {len(all_image_links)} images!\")\n",
    "\n",
    "#     # Save as JSON\n",
    "#     with open(\"all_image_links_flat.json\", \"w\") as f:\n",
    "#         json.dump(all_image_links, f, indent=4)\n",
    "\n",
    "#     print(\"\\nâœ… Saved to all_image_links_flat.json!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3bf5dc-c22d-4388-bede-02332fd4977c",
   "metadata": {},
   "source": [
    "## Clean Master File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109ec062-b996-4ff9-8598-740c301039e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file = pd.read_csv(\"wildcam_fulldata_2019.csv\", parse_dates=['date', 'datetime'])\n",
    "\n",
    "master_file = master_file.rename(columns=lambda x: x.strip())\n",
    "\n",
    "print(f\"âœ… Rows before cleaning 'filename': {len(master_file)}\")\n",
    "\n",
    "master_file['filename'] = master_file['filename'].str.strip()\n",
    "master_file['species'] = master_file['species'].str.capitalize()\n",
    "\n",
    "# Step 1: Drop rows where filename is actually NaN (not a string)\n",
    "master_file = master_file[master_file['filename'].notna()]\n",
    "\n",
    "# Step 2: Now strip and drop empty or whitespace-only filenames\n",
    "master_file['filename'] = master_file['filename'].str.strip()\n",
    "master_file = master_file[master_file['filename'] != '']\n",
    "\n",
    "# Confirm\n",
    "print(f\"âœ… Remaining rows after cleaning 'filename': {len(master_file)}\")\n",
    "\n",
    "# Drop exact duplicates across all columns, keep only the first occurrence\n",
    "master_file = master_file.drop_duplicates(keep='first')\n",
    "\n",
    "# Confirm the result\n",
    "print(f\"âœ… Rows remaining after dropping exact duplicates: {len(master_file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ebda04-60f4-4836-8d44-c0f01158628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_file_first_half = master_file[~master_file['year_start'].isin([2018, 2019])]\n",
    "# master_file_second_half = master_file[master_file['year_start'].isin([2018, 2019])]\n",
    "# print(f\"âœ… Total Number of Rows in master_file_first_half: {len(master_file_first_half)}\")\n",
    "# print(f\"âœ… Total Number of Rows in master_file_second_half: {len(master_file_second_half)}\")\n",
    "\n",
    "# total_split = len(master_file_first_half) + len(master_file_second_half)\n",
    "# original_total = len(master_file)\n",
    "\n",
    "# if total_split == original_total:\n",
    "#     print(f\"âœ… Split is complete: {total_split} rows match the original {original_total}.\")\n",
    "# else:\n",
    "#     print(f\"âŒ Mismatch: split total = {total_split}, original = {original_total}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429a9d56-9137-454b-8311-16843d56f4f2",
   "metadata": {},
   "source": [
    "## Clean Additional File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0182fd6-bf04-4194-b288-19f7dacedf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_file = pd.read_csv(\"wildcam_year3_additionaldata.csv\", parse_dates=['DateTimeOriginal', 'DateTimeCorrected'])\n",
    "\n",
    "add_file = add_file.rename(columns=lambda x: x.lower())\n",
    "add_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627781f8-2e3a-4234-9d34-cae9362c00e8",
   "metadata": {},
   "source": [
    "## Combine JSON files into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2544e1e-6cb9-419f-9351-8e3b5d32b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(json_path):\n",
    "    \"\"\"\n",
    "    Converts a nested JSON {topic: {id: text}} into a flat pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns ['topic', 'id', 'text']\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cf15c7-c2d1-4e40-8d01-b97640c63ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016_2017 = json_to_df(\"all_image_links_2016_2017.json\")\n",
    "df_2017_2018 = json_to_df(\"all_image_links_2017_2018.json\")\n",
    "df_2018_2019_1 = json_to_df(\"all_image_links_2018_2019_1.json\")\n",
    "df_2018_2019_2 = json_to_df(\"all_image_links_2018_2019_2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e70465-41cf-47bd-95bc-ae56ffdee0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([df_2016_2017, df_2017_2018, df_2018_2019_1, df_2018_2019_2])\n",
    "\n",
    "print(f\"âœ… Total Number of Rows: {len(full_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab023b9-b508-447f-9681-8b5dc3e609c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make sure the column exists and is clean\n",
    "full_df['filename'] = full_df['filename'].astype(str).str.strip()\n",
    "\n",
    "# Step 1: Count how many match the _original pattern\n",
    "num_originals = full_df[full_df['filename'].str.endswith('_original')].shape[0]\n",
    "print(f\"ðŸ§® Rows ending with _original.JPG: {num_originals}\")\n",
    "\n",
    "# Step 2: Remove those rows\n",
    "full_df = full_df[~full_df['filename'].str.endswith('_original')]\n",
    "\n",
    "# Step 3: Check the new shape\n",
    "print(f\"âœ… Remaining rows after 'original' removal: {len(full_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267071a5-c6b0-4a7d-b1e3-fc55b2933d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean 'directory'\n",
    "full_df.loc[:, 'directory'] = full_df['directory'].astype(str).str.strip()\n",
    "\n",
    "# Create 'site' and 'species'\n",
    "full_df.loc[:, 'site'] = full_df['directory'].apply(lambda x: x.split('/')[0])\n",
    "full_df.loc[:, 'species'] = full_df['directory'].apply(lambda x: x.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ecea4-2782-43a6-aeae-ab060cb41868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_species(name):\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "\n",
    "    name = name.strip()\n",
    "\n",
    "    # 1. Remove trailing numeric suffixes like \" 2\"\n",
    "    name = re.sub(r\"\\s+2$\", \"\", name)\n",
    "\n",
    "    # 2. Replace inconsistent naming\n",
    "    if name == \"Mongoose_white tailed\":\n",
    "        name = \"Mongoose_white_tailed\"\n",
    "        \n",
    "    if name == \"Mongoose_large_gray\":\n",
    "        name = \"Mongoose_large_grey\"\n",
    "        \n",
    "    if name == \"Samango\":\n",
    "        name = \"Samango_monkey\"\n",
    "        \n",
    "    if name == \"Mongoose_other\":\n",
    "        name = \"Mongoose\"\n",
    "\n",
    "    if name == \"Vervet\":\n",
    "        name = \"Vervet_monkey\"\n",
    "        \n",
    "    if name == \"Hippo\":\n",
    "        name = \"Hippopotamus\"\n",
    "        \n",
    "    if name == \"Hornbill_ground\":\n",
    "        name = \"Ground_hornbill\"\n",
    "        \n",
    "    if name in ['Snake', 'Lizard', 'Monitor_lizard', 'Reptile']:\n",
    "        name = 'Reptile_amphibian'\n",
    "\n",
    "    # 3. Remove '_unknown' from names like 'Mongoose_unknown', 'Duiker_unknown'\n",
    "    name = re.sub(r\"_unknown$\", \"\", name)\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772052e3-8d52-43c1-adb9-308ba3a253a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_df = extract_datetime_from_filename(full_df)\n",
    "# Apply the updated function\n",
    "full_df['species'] = full_df['species'].apply(standardize_species)\n",
    "\n",
    "full_df.to_csv('full_df.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb779f64-f75f-476b-852e-979116043f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dt = full_df[full_df[['date', 'time']].isnull().any(axis=1)]\n",
    "missing_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f410309-76db-4433-9aaa-e8e4149f4d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_USE_MTLS_ENDPOINT\"] = \"never\"\n",
    "os.environ[\"GOOGLE_API_USE_UNCACHED_DISCOVERY_DOCS\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c95a6f3-912f-4c71-a33a-8982a1e56e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicated filepath values\n",
    "duplicated_filepaths = full_df[full_df.duplicated(subset='filepath', keep=False)]\n",
    "\n",
    "# Count them\n",
    "print(f\"ðŸ” Number of rows with duplicated filepath: {len(duplicated_filepaths)}\")\n",
    "\n",
    "# Preview some\n",
    "duplicated_filepaths.sort_values('filepath').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15a2b83-ff67-4f70-b335-491b6b65f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_directory(path):\n",
    "    if pd.isna(path):\n",
    "        return path\n",
    "    parts = path.strip(\"/\").split(\"/\")\n",
    "    return \"/\".join(parts[-2:]) if len(parts) >= 2 else path\n",
    "\n",
    "# Apply the function to the directory column\n",
    "master_file['directory'] = master_file['directory'].apply(simplify_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8d83ed-7c01-499f-9746-38ccba46c6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: List columns from master_file to include (exclude 'directory' but keep join keys)\n",
    "# cols_to_merge = [col for col in master_file_first_half.columns if col != 'directory' or col in ['filename', 'site', 'species', 'year_start']]\n",
    "\n",
    "# Step 2: Perform the merge\n",
    "merged_df_1 = pd.merge(\n",
    "    full_df,\n",
    "    master_file,\n",
    "    on=['filename', 'directory'],\n",
    "    how='inner'\n",
    ")\n",
    "merged_df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8fa493-f822-4d8e-b998-257c9b04dd9a",
   "metadata": {},
   "source": [
    "## Match jsons with master file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd26bb1-3544-4c4b-a754-22861a47139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # step 1\n",
    "\n",
    "# python -m megadetector.detection.run_detector_batch \n",
    "# MDV5A \n",
    "# ~/Desktop/Kaitlyn_Catalyst/ct_classifier/datasets/CaltechCT/eccv_18_all_images_sm \n",
    "# ~/Desktop/Kaitlyn_Catalyst/ct_classifier/datasets/CaltechCT/eccv_18_all_images_sm.json \n",
    "# --output_relative_filenames \n",
    "# --recursive \n",
    "# --checkpoint_frequency 10000 \n",
    "# --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a6245e-3688-4892-8919-44c3f48c7325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # step 2\n",
    "\n",
    "# python -m megadetector.separate_detections_into_folders.py \n",
    "# Desktop/Kaitlyn_Catalyst/ct_classifier/datasets/CaltechCT/eccv_18_all_images_sm.json \n",
    "# Desktop/Kaitlyn_Catalyst/ct_classifier/datasets/CaltechCT/eccv_18_all_images_sm \n",
    "# Desktop/Kaitlyn_Catalyst/ct_classifier/datasets/CaltechCT/separated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e55ec50-34ce-446d-bb73-3ea68e498bee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv4ecology",
   "language": "python",
   "name": "cv4ecology"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
